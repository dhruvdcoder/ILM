import sys
import os
import argparse
from datasets import load_dataset
from llm_eval import LLMEval
from pathlib import Path

# Grammaticality
# Coherence
# Humanlikeness
# Fluency
# Consistency
# Readability
# Nonredundancy
# Spelling accuracy

grammaticality_rubric = """
(Does the text demonstrate proper grammatical usage?)
Score of 1: The text contains frequent grammatical errors, making it difficult to understand.
Score of 2: The text shows occasional grammatical errors, which disrupt the flow and clarity of the text.
Score of 3: The text generally adheres to grammatical rules, though minor errors are present.
Score of 4: The text demonstrates good grammaticality with rare errors that do not affect comprehension.
Score of 5: The text excels in grammatical usage, with clear and correct grammar throughout.
"""

coherence_rubrics = """
(Is the text coherent and logically organized?)
Score of 1: Very incoherent. The generation lacks structure, has sudden jumps, and is difficult to follow.
Score of 2: Somewhat incoherent. The generation has some semblance of structure, but has significant flaws in flow and organization.
Score of 3: Neutral. The generation is decently organized, with minor issues in flow and structure.
Score of 4: Mostly coherent. The generation is well-structured with very few minor coherence issues.
Score of 5: Highly coherent. The generation is excellently organized, flows seamlessly, and builds information logically from start to end.
"""

fluency_rubrics = """
(Is the text fluent and easy to read?)
Score of 1: The text is disjointed and lacks fluency, making it hard to follow.
Score of 2: The text has limited fluency with frequent awkward phrasing.
Score of 3: The text is moderately fluent, with some awkward phrasing but generally easy to follow.
Score of 4: The text is fluent with smooth transitions and rare awkward phrases.
Score of 5: The text is highly fluent, with natural and smooth expression throughout.
"""

consistency_rubrics = """
(Is the text consistent in terms of style, tone, and tense?)
Score of 1: The text is inconsistent in style, tone, and tense, leading to confusion.
Score of 2: The text shows occasional inconsistencies in style, tone, and tense.
Score of 3: The text is mostly consistent in style, tone, and tense, with minor lapses.
Score of 4: The text is consistent in style, tone, and tense, with rare inconsistencies.
Score of 5: The text is highly consistent in style, tone, and tense throughout.
"""

humanlikeness_rubric = """
(Does the text resemble natural human writing and expression?)
Score of 1: The text is highly unnatural, robotic, or incoherent, making it clear that it was generated by machine.
Score of 2: The text has some human-like qualities but contains unnatural phrasing, repetition, or rigid structure.
Score of 3: The text is moderately human-like, with some mechanical tendencies but generally reasonable expression.
Score of 4: The text is mostly human-like, with natural flow and expression, though minor artifacts of machine generation may still be present.
Score of 5: The text is highly human-like, indistinguishable from naturally written text, with fluid, nuanced, and contextually appropriate expression.
"""

nonredundancy_rubric = """
(Does the text avoid unnecessary repetition?)
Score of 1: The text is highly redundant, with excessive repetition of words, phrases, or ideas that make it difficult to read.  
Score of 2: The text contains noticeable redundancy, with multiple instances of unnecessary repetition that affect clarity.  
Score of 3: The text has some minor redundancy, but it does not significantly impact readability or meaning.  
Score of 4: The text is mostly nonredundant, with rare instances of repetition that do not affect clarity.  
Score of 5: The text is highly concise and avoids any unnecessary repetition, presenting ideas efficiently and effectively.  
"""

spelling_accuracy_rubric = """
(Does the text demonstrate correct spelling? Just focus on spelling errors and not the grammar.)  
Score of 1: The text contains frequent spelling errors, making it difficult to understand.  
Score of 2: The text has multiple spelling errors that affect readability and clarity.  
Score of 3: The text has occasional spelling errors, but they do not significantly impact comprehension.  
Score of 4: The text is mostly free of spelling errors, with only rare mistakes that do not affect understanding.  
Score of 5: The text has perfect spelling accuracy, with no errors present.  
"""

readability_rubric = """
(Is the text easy to read and understand?)  
Score of 1: The text is very difficult to read due to complex wording, poor structure, or excessive jargon.  
Score of 2: The text is somewhat difficult to read, with frequent awkward phrasing or unnecessarily complex language.  
Score of 3: The text is moderately readable, with occasional awkward phrasing but generally understandable.  
Score of 4: The text is easy to read, with clear wording and smooth sentence structure.  
Score of 5: The text is highly readable, with a natural flow, clear structure, and an engaging style.  
"""

def parse_args():
    parser = argparse.ArgumentParser(description="Run LLMEval on a dataset.")
    parser.add_argument('--input_file', type=str, required=True, help="Path to the input JSON file.")
    parser.add_argument('--output_file', type=str, required=True, help="Path to the output file for results.")
    parser.add_argument('--model_repo', type=str, required=True, help="Name of the model repository for evaluation.")
    parser.add_argument('--rubrics_name', type=str, required=True, help="Name of the rubrics.")
    return parser.parse_args()

def get_rubrics(rubric_name):
    if rubric_name == "coherence":
        return coherence_rubrics
    elif rubric_name == "grammaticality":
        return grammaticality_rubric
    elif rubric_name == "fluency":
        return fluency_rubrics
    elif rubric_name == "consistency":
        return consistency_rubrics
    elif rubric_name == "humanlikeness":
        return humanlikeness_rubric
    elif rubric_name == "nonredundancy":
        return nonredundancy_rubric
    elif rubric_name == "spelling_accuracy":
        return spelling_accuracy_rubric
    elif rubric_name == "readability":
        return readability_rubric
    else:
        return ""

if __name__ == "__main__":
    args = parse_args()
    output_path = Path(args.output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    dataset = load_dataset("json", data_files=args.input_file)
    dc = LLMEval(dataset=dataset["train"], rubrics=get_rubrics(args.rubrics_name), output_file=args.output_file, model_repo=args.model_repo, min_score=1, max_score=5)    
    dc.generate_inference_file()
