defaults:
  - _self_
  - paths: default
  - hydra: default
  # Pull in the defaults for scaffolding
  - callbacks: default
  - datamodule: default
  - loggers: default
  - generative_perplexity@generative_perplexity.evaluators: [gpt2-small] # TODO (mdlm): Check MDLM and set the right model here.
  - trainer_strategy: single_device
  # experiment specific global edits
  - model: ??? # default
  - length_model: null
  - noise_schedule: loglinear # default
  - dataset: ??? # babylm # will set datamodule
  - collation_strategy: ??? # pad
  - model_type: ??? # discrete_time_absorbing
  # specific changes
  - type: null
  - experiment: null
  - debug: null

# example usage: model=absorbing_ddit_small noise_schedule=discrete_time_linear dataset=babylm collation_strategy=pad model_type=discrete_time_absorbing

# Keep common utils
hydra:
  job:
    env_set:
      PROJECT_ROOT: "."
      OMP_NUM_THREADS: "1"
  searchpath:
    - file://${oc.env:PROJECT_ROOT}/configs/common


# main config that controls the components
seed: 1

job_type: train
job_name: job_type=${job_type}__model_type=${tags.model_type}__model=${tags.model}__scheduler=${tags.noise_schedule}__dataset=${tags.dataset}__tokenizer=${tags.tokenizer}__collation=${tags.collation_strategy}__debug=${tags.debug}
global_flags: null

checkpointing_dir: ${paths.output_dir}/checkpoints
resume_from_checkpoint: true
resume_checkpoint_path: null
compile: false



# components

trainer:
  _target_: lightning.Trainer
  accelerator: cuda
  num_nodes: 1
  strategy: auto
  devices: ${device_count:}
  max_steps: 1000_000 # 1M steps # TODO (config): make this dynamically calculated
  default_root_dir: ${paths.output_dir}
  accumulate_grad_batches: ${find_grad_accum:${datamodule.global_batch_size},${datamodule.train_dataloader_kwargs.batch_size},${.devices},${.num_nodes}}
  gradient_clip_val: 1.0
  log_every_n_steps: 50
  val_check_interval: 50000
  # TODO (efficiency): precision bf16 or other

optimizer:
  _target_: torch.optim.AdamW
  lr: 0.0005
  weight_decay: 0.01

lr_scheduler:
  name: "cosine"
  #fraction_warmup_steps: 0.002 # 2000 for 1M steps
  num_warmup_steps: 1000
  num_training_steps: ${trainer.max_steps}
  monitor: "train/loss"

generative_perplexity:
  num_samples: 64 # total samples
  evaluators: null

tags:
  debug: false
  model_type: ???
  model: ???
  dataset: ???
  collation_strategy: ???
  noise_schedule: ???
  job_type: ${job_type}