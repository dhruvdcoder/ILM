
# for generating for ANLG using a model trained on LM1B

_target_: pcdd.datamodule.anlg.ANLGForILMStoppingDataModule
tokenizer:
  _target_: pcdd.datamodule.datamodule.BertTokenizerForILMFast.from_pretrained
  pretrained_model_name_or_path: bert-base-uncased
manual_cache_dir: ${oc.env:DATA_DIR, data}
noise_schedule: ${noise_schedule}
num_dataset_workers: 4
block_size: 128
global_batch_size: 64
predict_dataloader_kwargs:
  batch_size: ${..global_batch_size}
  num_workers: 0
  shuffle: false 
  pin_memory: false
  persistent_workers: false
  drop_last: false
