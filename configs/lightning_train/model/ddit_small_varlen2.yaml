# @package _global_


model:
  _target_: pcdd.models.ddit_var_len.DDitVarLenModel
  num_embeddings: ${tokenizer:vocab_size}
  d_model: 768 # mdlm
  num_layers: 12
  nhead: 12
  padding_idx: ${tokenizer:pad_token_id}
  mask_idx: ${tokenizer:mask_token_id}
  dim_feedforward: ${eval:${.d_model}*4}
  dropout: 0.1
  activation: "relu"
  layer_norm_eps: 1e-5
  d_cond: 128 # mdlm
  max_length: ${predictor.max_length}
  force_flash_attn: false

length_model:
  _target_: pcdd.models.ddit_var_len.SimpleEmbeddingLengthModel
  max_output_length: ${predictor.max_length}
  input_dim: ${model.d_model}

tags:
  model: ddit_small_varlen2
  length_model: logits